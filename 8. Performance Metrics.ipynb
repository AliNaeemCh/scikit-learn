{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/fraud_detection.csv')\n",
    "X = df.iloc[:, 1:-1]\n",
    "y = df['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "total_training_examples = X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(X_train)\n",
    "X_train = mm.transform(X_train)\n",
    "X_test = mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = math.floor(math.sqrt(total_training_examples))\n",
    "knn = KNeighborsClassifier(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=127)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=127)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=127)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Confusion Matrix\n",
    "![Image](images/confusion-matrix.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2883  110]\n",
      " [ 456  645]] \n",
      "\n",
      "True positive (Actually positive, classified as positive): 2883\n",
      "False positive (Actually negative, classified as positive): 110\n",
      "False negative (Actually positive, classified as negative): 456\n",
      "True negative (Actually negative, classified as negative): 645\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm, '\\n')\n",
    "print('True positive (Actually positive, classified as positive):', cm[0][0])\n",
    "print('False positive (Actually negative, classified as positive):', cm[0][1])\n",
    "print('False negative (Actually positive, classified as negative):', cm[1][0])\n",
    "print('True negative (Actually negative, classified as negative):', cm[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy gives an estimate of the number of correct decisions made by the classifier.\n",
    "\n",
    "Formula:\n",
    "$$Accuracy = \\frac{\\text{Correct Decisions}}{\\text{Total Decisions}}$$\n",
    "\n",
    "For skewed datasets, the accuracy may not give a fair estimate of the algorithm's performance. For instance, suppose in a binary classification problem, the class 1 has 2 instances and class 0 has 98 instances and regardless of the input, the algorithm classifies it as instance of class 0. In this case, out of the total 100 decisions, 98 will be correct and 2 will be wrong, giving an accuracy of 98%. But we know that this classifier couldn't be more worse. For such scenarios, the concepts of precision and recall come to rescue.\n",
    "\n",
    "### Precision\n",
    "Precision is the measure of correctness of a prediction that if an algorithm predicted a particular class a certain number of times, how many times it was correct.\n",
    "\n",
    "Formula:\n",
    "$$Precision = \\frac{\\text{Correct Positive}}{\\text{Correct Positive} + \\text{Wrong Positive}}$$\n",
    "\n",
    "- A High Precision means when the algorithm did predict a particular class, mostly it was correct.\n",
    "- A Low Precision means when the algorithm did predict a particular class, mostly it was wrong.\n",
    "\n",
    "Notice that, precision doesn't tell anything about how many times the algorithm did predict a particular class when it should have. For instance, let's say the algorithm predicted a particular class 10 times and all those instances actually belonged to that class. In this case, the precision is 100%. You would think that this is the best algorithm, but here's the catch: The total instances of that class were 100 and the algorithm only predicted 10 of them and misclassified the 90 instances, which is not a good thing, for sure. To estimate the latter aspect, we use Recall.\n",
    "\n",
    "### Recall\n",
    "Recall is the measure of how well the algorithm is able to identify a particular class, that if a particular class had a certain number of instances, how many times the algorithm was able to identify them. \n",
    "\n",
    "Formula:\n",
    "$$Recall = \\frac{\\text{Correct Positive}}{\\text{Actual Positive}}$$\n",
    "\n",
    "\n",
    "- A High Recall means the algorithm was able to identify a particular class instances well.\n",
    "- A Low Recall means the algorithm couldn't identify a particular class instances well.\n",
    "\n",
    "If we advance our previous example with 100 instances of a particular class and the algorithm identified correctly 10 of them, giving a precision of 100%, the recall would be very low and that is 10/100 = 10%.\n",
    "\n",
    "That's how we can use precision and recall together to get insight into the algorithm's performance.\n",
    "\n",
    "### The Precision and Recall Tradeoff\n",
    "In practical scenarios, there's a tradeoff between precision and recall. Here's how:\n",
    "- A high precision ensures that most of the predictions of a particular class are correct. This means that the algorithm will only predict that class when the sureity or confidence is very high. This implies that if the confidence is low, the algorithm won't predict that class but it's possible that an instance with relatively low confidence does belong to that class. This results in many instances, which do belong to a particular class, not correctly classified by the algorithm, constituting low recall.\n",
    "\n",
    "- A high recall ensures that most of the instances of a particular class are correctly identified by the algorithm. This is possible when the algorithm predicts that class even if the confidence is little bit low. But this will result in many instances classified to belong to a particular class when they don't actually, yielding in a low precision.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/precision-recall-tradeoff.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "You would want to have a high precision (high confidence or threshold) when it's more important that the predicted class is correct than class being predicted at all. This could be the case when a particular disease treatment is very expensive and invasive, however, if not treated, isn't life threatening.\n",
    "\n",
    "Conversely, you would want to have a high recall (relatively low threshold/confidence is applicable) when it's more important to predict a particular class than the predicted class being correct. This could be the case when a particular disease is life threatening and the risk of leaving it untreated can't be taken.\n",
    "\n",
    "### The F1 Score\n",
    "The F1 score is the harmonic mean of precision and recall which gives a performance metric having effect of both precision and recall. The harmonic mean is highly influenced by small values, so if any of precision or recall is low, the F1 score will also be hit.\n",
    "\n",
    "Formula:\n",
    "$$\\text{F1 Score} = 2\\cdot\\frac{\\text{Precision}\\times\\text{Recall}}{\\text{Precision + Recall}}$$\n",
    "\n",
    "### Support\n",
    "Support is the number of instances, samples or examples of a particular class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      2993\n",
      "           1       0.85      0.59      0.70      1101\n",
      "\n",
      "    accuracy                           0.86      4094\n",
      "   macro avg       0.86      0.77      0.80      4094\n",
      "weighted avg       0.86      0.86      0.85      4094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(y_test, y_pred)\n",
    "print('Classification Report\\n', cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report Explanation:\n",
    "- The precision and recall is shown separately for all classes i-e 0 and 1.\n",
    "- Both precision and recall is decent for class 0, which means that the algorithm performs good when it comes to classifying class 0 correctly.\n",
    "- The precision of algorithm for class 1 is high, however, the recall is low, which means that the when algorithm predicts class 1, it is correct most of the time. But the algorithm misclassifies or fails to identify correctly many instances of class 1.\n",
    "- Support of 2993 for class 0 is the number of instances of class 0 in test set.\n",
    "- Support of 1101 for class 1 is the number of instances of class 1 in test set.\n",
    "- Accuracy of 86% is calulated by the ratio of correct decisions to the total decisions.\n",
    "- The support of 4094 in the last 3 rows is the sum of instances of all the classes in the test set (2993+1101).\n",
    "- Macro avg is calculated by simply adding the values of a particular column for all classes, divided by the total number of classes.\n",
    "- Weighted avg is calculated using the support as weights. For instance, weighted avg of F1 score is calculated as:\n",
    "\n",
    "$$\\text{F1 Score (Weighted Avg)} = \\frac{2993\\times 0.91 + 1101\\times 0.70}{2993+1101} = 0.85$$\n",
    "\n",
    "The weighted average of F1 score is the most reasonable parameter in this classification report. However, the precision and recall of individual classes should also be analyzed, especially of those classes for whom accuracy is critical.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
